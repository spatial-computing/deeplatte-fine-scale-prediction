{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train__embedding_similar_binned_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import dapm\n",
    "from scripts.data_loader import *\n",
    "from scripts.train_dapm import train\n",
    "from utils.metrics import normalize_mat\n",
    "from params import Param\n",
    "from utils.logging_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dapm_main(param, **kwargs):\n",
    "    \n",
    "    \"\"\" define model name \"\"\" \n",
    "    model_name = param.generate_model_name()\n",
    "#     print(model_name)\n",
    "    ae_model_name = param.generate_ae_model_name()\n",
    "#     print(ae_model_name)\n",
    "\n",
    "    \"\"\" define prevoius model name for fine tuning \"\"\" \n",
    "    model_files = os.listdir(kwargs['model_dir'])\n",
    "    previous_model_name = ''\n",
    "    for f in os.listdir(kwargs['model_dir']):\n",
    "        if f'{param.last_year}___#{param.last_month}#' in f and '.pkl' in f:\n",
    "            previous_model_name = f\n",
    "    if len(previous_model_name) == 0:\n",
    "        return\n",
    "\n",
    "    print(model_name)\n",
    "    print(previous_model_name)\n",
    "    model = torch.load(os.path.join(kwargs['model_dir'], previous_model_name)).to(kwargs['device'])\n",
    "\n",
    "    kwargs['model_name'] = model_name\n",
    "    kwargs['model_file'] = os.path.join(kwargs['model_dir'], model_name + '.pkl')\n",
    "    kwargs['log_file'] = os.path.join(kwargs['log_dir'], model_name + '.log')\n",
    "    kwargs['run_file'] = os.path.join(kwargs['run_dir'], model_name + '_run_{}'.format(datetime.datetime.now().strftime('%d%H%m')))\n",
    "    kwargs['ae_model_file'] = os.path.join('./data/ae_models_2/models/', ae_model_name + '.pkl')\n",
    "\n",
    "    \"\"\" load data \"\"\"\n",
    "    data_dir = f'/home/yijun/notebooks/training_data/'\n",
    "    data_obj = load_data(data_dir, param)\n",
    "    train_loc, val_loc, test_loc = load_locations(kwargs['train_val_test'], param)\n",
    "    \n",
    "    data_obj.train_loc = train_loc\n",
    "    data_obj.train_y = data_obj.gen_train_val_test_label(data_obj.label_mat, data_obj.train_loc)\n",
    "    data_obj.val_loc = val_loc\n",
    "    data_obj.val_y = data_obj.gen_train_val_test_label(data_obj.label_mat, data_obj.val_loc)\n",
    "    data_obj.test_loc = test_loc\n",
    "    data_obj.test_y = data_obj.gen_train_val_test_label(data_obj.label_mat, data_obj.test_loc)\n",
    "    \n",
    "    \"\"\" logging starts \"\"\"\n",
    "    start_logging(kwargs['log_file'], model_name)\n",
    "    data_logging(data_obj)\n",
    "\n",
    "    \"\"\" load ae model \"\"\"\n",
    "#     ae = torch.load(kwargs['ae_model_file'])\n",
    "    \n",
    "    \"\"\" define DeepAP model\n",
    "    in_dim, ae_en_h_dims, ae_de_h_dims\n",
    "    conv_lstm_in_size, conv_lstm_in_dim, conv_lstm_h_dim, conv_lstm_kernel_sizes, conv_lstm_n_layers\n",
    "    fc_in_dim, fc_h_dims, fc_out_dim  \"\"\"\n",
    "#     model = dapm.DeepAPM(in_dim=data_obj.n_features,\n",
    "#                          ae_en_h_dims=param.ae_en_h_dims,\n",
    "#                          ae_de_h_dims=param.ae_de_h_dims,\n",
    "                               \n",
    "#                          conv_lstm_in_size=(data_obj.n_rows, data_obj.n_cols),\n",
    "#                          conv_lstm_in_dim=param.ae_en_h_dims[-1],  \n",
    "#                          conv_lstm_h_dim=[param.dapm_h_dim],  # dap_h_dim\n",
    "#                          conv_lstm_kernel_sizes=param.kernel_sizes,  # kernel_sizes\n",
    "#                          conv_lstm_n_layers=1,\n",
    "                               \n",
    "#                          fc_in_dim=param.dapm_h_dim * len(param.kernel_sizes),\n",
    "#                          fc_h_dims=param.fc_h_dims,  # fc_h_dims\n",
    "#                          fc_out_dim=1,\n",
    "                                    \n",
    "#                          ae_pretrain_weight=ae.state_dict(),\n",
    "#                          mask_thre=param.mask_thre,\n",
    "#                          fc_p_dropout=0.1,\n",
    "#                          device=kwargs['device'])\n",
    "#    \n",
    "#     model = model.to(kwargs['device'])\n",
    "    \n",
    "#     model = torch.load(f'data/los_angeles_500m_separate_1234_tp1/models/dapm___sp_ae_sc___{param.area}_{param.resolution}m_{param.year}___#{param.months[0]}#___6_00001_1___05_01_5_0___16_13.pkl')\n",
    "#     model = torch.load(f'data/dapm_models/models/{param.area}_{param.resolution}m_{param.year}___#{param.months[0]}#.pkl')\n",
    "#     model = model.to(kwargs['device'])\n",
    "#     for p in model.parameters():\n",
    "#         p.requires_grad = True\n",
    "    \n",
    "    train(model, data_obj, param, **kwargs)\n",
    "\n",
    "    \"\"\" logging ends \"\"\"\n",
    "    end_logging(model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gaussian(h, r, s, n=0):\n",
    "    return n + s * (1. - np.exp(- (h ** 2 / (r / 2.) ** 2)))\n",
    "\n",
    "def exponential(h, r, s, n=0):\n",
    "    return n + s * (1. - np.exp(-(h / (r / 3.))))\n",
    "\n",
    "def get_fit_bounds(x, y):\n",
    "    n = np.nanmin(y)\n",
    "    r = np.nanmax(x)\n",
    "    s = np.nanmax(y)\n",
    "    return (0, [r, s, n])\n",
    "\n",
    "\n",
    "def get_fit_func(x, y, model):\n",
    "    try:\n",
    "        bounds = get_fit_bounds(x, y)\n",
    "        popt, _ = curve_fit(model, x, y, method='trf', p0=bounds[1], bounds=bounds)\n",
    "        return popt\n",
    "    except Exception as e:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "\n",
    "def gen_semivariogram(distances, variances, bins, thr):\n",
    "        \n",
    "    valid_variances, valid_bins = [], []\n",
    "    for b in range(len(bins) - 1):\n",
    "        left, right = bins[b], bins[b + 1]\n",
    "        mask = (distances >= left) & (distances < right)\n",
    "        if np.count_nonzero(mask) > 10:\n",
    "            v = np.nanmean(variances[mask])\n",
    "            d = np.nanmean(distances[mask])\n",
    "            valid_variances.append(v)\n",
    "            valid_bins.append(d)\n",
    "            \n",
    "    x, y = np.array(valid_bins), np.array(valid_variances)\n",
    "    popt = get_fit_func(x, y, model=gaussian)                        \n",
    "    return popt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dat\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import autograd\n",
    "\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.metrics import compute_error\n",
    "from models.spatial_loss_func import SpatialLossFunc\n",
    "\n",
    "        \n",
    "def train(dapm, data_obj, args, **kwargs):\n",
    "    \n",
    "    \"\"\" construct index-based data loader \"\"\"\n",
    "    idx = np.array([i for i in range(args.seq_len + 1, data_obj.train_y.shape[0])])\n",
    "    idx_dat = dat.TensorDataset(torch.tensor(idx, dtype=torch.int32))\n",
    "    train_idx_data_loader = dat.DataLoader(dataset=idx_dat, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    idx = np.array([i for i in range(args.seq_len + 1, data_obj.test_y.shape[0])])\n",
    "    idx_dat = dat.TensorDataset(torch.tensor(idx, dtype=torch.int32))\n",
    "    test_idx_data_loader = dat.DataLoader(dataset=idx_dat, batch_size=1, shuffle=False)\n",
    "\n",
    "    \"\"\" set writer, loss function, and optimizer \"\"\"\n",
    "    writer = SummaryWriter(kwargs['run_file'])\n",
    "    loss_func = nn.MSELoss()\n",
    "    spatial_loss_func = SpatialLossFunc(sp_neighbor=args.sp_neighbor) \n",
    "    optimizer = optim.Adam(dapm.parameters(), lr=args.lr, weight_decay=1e-8)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "    \n",
    "    def get_current_learning_rate(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "        \n",
    "    def adjust_learning_rate(optimizer, epoch):\n",
    "        \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "        if epoch > 20:\n",
    "            lr = args.lr * (0.1 ** (epoch // 10))\n",
    "        else:\n",
    "            lr = get_current_learning_rate(optimizer)\n",
    "        print(lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        dapm.train()\n",
    "        total_losses, train_losses, val_losses, l1_losses, ae_losses, ac_losses, tp_losses, sp_losses = [], [], [], [], [], [], [], []\n",
    "\n",
    "#         adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        for _, idx in enumerate(train_idx_data_loader):\n",
    "            batch_idx = idx[0]\n",
    "\n",
    "            ############################\n",
    "            # construct sequence input #\n",
    "            ############################\n",
    "\n",
    "            def construct_sequence_x(idx_list, dynamic_x, static_x):\n",
    "                d_x = [dynamic_x[i - args.seq_len: i + 1, ...] for i in idx_list]\n",
    "                d_x = np.stack(d_x, axis=0)\n",
    "                s_x = np.expand_dims(static_x, axis=0)\n",
    "                s_x = np.repeat(s_x, args.seq_len + 1, axis=1)  # (t, c, h, w)\n",
    "                s_x = np.repeat(s_x, len(idx_list), axis=0)  # (b, t, c, h, w)\n",
    "                x = np.concatenate([d_x, s_x], axis=2)\n",
    "                return torch.tensor(x, dtype=torch.float).to(kwargs['device'])\n",
    "\n",
    "            def construct_y(idx_list, output_y):\n",
    "                y = [output_y[i] for i in idx_list]\n",
    "                y = np.stack(y, axis=0)\n",
    "                return torch.tensor(y, dtype=torch.float).to(kwargs['device'])\n",
    "\n",
    "            batch_x = construct_sequence_x(batch_idx, data_obj.dynamic_x, data_obj.static_x)  # x = (b, t, c, h, w)\n",
    "            batch_y = construct_y(batch_idx, data_obj.train_y)  # y = (b, c, h, w)\n",
    "            batch_val_y = construct_y(batch_idx, data_obj.val_y)\n",
    "\n",
    "            ###################\n",
    "            # train the model #\n",
    "            ###################\n",
    "\n",
    "            out, masked_x, _, de_x, em = dapm(batch_x)\n",
    "            train_loss = loss_func(batch_y[~torch.isnan(batch_y)], out[~torch.isnan(batch_y)])\n",
    "            train_losses.append(train_loss.item())\n",
    "\n",
    "            # add loss according to the model type\n",
    "            total_loss = train_loss\n",
    "            if 'sp' in args.model_type:\n",
    "                mask_layer_params = torch.cat([x.view(-1) for x in dapm.mask_layer.parameters()])\n",
    "                l1_regularization = torch.norm(mask_layer_params, 1)\n",
    "                l1_losses.append(l1_regularization.item())\n",
    "                total_loss += l1_regularization * args.alpha\n",
    "\n",
    "            if 'ae' in args.model_type:\n",
    "                ae_loss = loss_func(masked_x, de_x)\n",
    "                ae_losses.append(ae_loss.item())\n",
    "                total_loss += ae_loss * args.gamma\n",
    "            \n",
    "            if 'sc' in args.model_type:\n",
    "                sp_loss = spatial_loss_func(out)\n",
    "                sp_losses.append(sp_loss.item())\n",
    "                total_loss += sp_loss * args.beta\n",
    "\n",
    "            if 'esc' in args.model_type:\n",
    "                # 1-step temporal neighboring loss\n",
    "                pre_batch_idx = batch_idx - torch.ones_like(batch_idx)\n",
    "                pre_batch_x = construct_sequence_x(pre_batch_idx, data_obj.dynamic_x, data_obj.static_x)  # x = (b, t, c, h, w)\n",
    "                _, _, _, _, pre_em = dapm(pre_batch_x)\n",
    "                tp_loss = torch.mean(torch.mean((em - pre_em) ** 2, axis=1))\n",
    "                \n",
    "                # 1-step spatial neighboring loss\n",
    "                sp_loss = 0.\n",
    "                sp_loss += torch.sum(torch.mean((em[..., 1:, 1:] - em[..., :-1, :-1]) ** 2, axis=1)) \n",
    "                sp_loss += torch.sum(torch.mean((em[..., :-1, 1:] - em[..., 1:, :-1]) ** 2, axis=1)) \n",
    "                sp_loss += torch.sum(torch.mean((em[..., 1:, :] - em[..., :-1, :]) ** 2, axis=1)) \n",
    "                sp_loss += torch.sum(torch.mean((em[..., :, 1:] - em[..., :, :-1]) ** 2, axis=1)) \n",
    "                sp_loss = sp_loss / args.batch_size / (em.shape[-1] - 1) / (em.shape[-2] - 1)\n",
    "                \n",
    "                tp_losses.append(tp_loss.item())\n",
    "                sp_losses.append(sp_loss.item())\n",
    "                total_loss += (tp_loss + sp_loss) * args.beta\n",
    "                \n",
    "            if 'acc' in args.model_type:\n",
    "                \n",
    "                for t in range(batch_x.shape[0]):\n",
    "                    # flatten embeddings, labels, and predictions\n",
    "#                     em_flatten = em.permute(1, 0, 2, 3).reshape(64, -1)  # [64, 27968]\n",
    "                    em_flatten = em[t, ...].reshape(64, -1)  # [64, 27968]\n",
    "                    y_flatten = torch.flatten(batch_y[t, ...])\n",
    "                    out_flatten = torch.flatten(out[t, ...])\n",
    "\n",
    "                    # pairs of labeled locations \n",
    "                    y_mask = (~torch.isnan(y_flatten)).nonzero().view(-1)\n",
    "                    y_mask_pairs = torch.combinations(y_mask)\n",
    "                    p1, p2 = y_mask_pairs[:, 0], y_mask_pairs[:, 1]\n",
    "                    y_em_sim = torch.mean((em_flatten[:, p1] - em_flatten[:, p2]) ** 2, dim=0)\n",
    "                    y_var = (y_flatten[p1] - y_flatten[p2]) ** 2 / 2\n",
    "\n",
    "                    out_mask = torch.randint(out_flatten.shape[0], (100,)).to(kwargs['device'])\n",
    "                    out_mask_pairs = torch.combinations(out_mask)\n",
    "                    p1, p2 = out_mask_pairs[:, 0], out_mask_pairs[:, 1]\n",
    "                    out_em_sim = torch.mean((em_flatten[:, p1] - em_flatten[:, p2]) ** 2, dim=0)\n",
    "                    out_var = (out_flatten[p1] - out_flatten[p2]) ** 2 / 2\n",
    "\n",
    "                    # rescale the distance\n",
    "                    def max_min_rescale(t, min_t, max_t):\n",
    "                        t -= min_t\n",
    "                        t /= (max_t - min_t)\n",
    "                        return t\n",
    "\n",
    "                    max_sim = torch.max(torch.cat([out_em_sim, y_em_sim])).detach()\n",
    "                    min_sim = torch.min(torch.cat([out_em_sim, y_em_sim])).detach()\n",
    "                    y_em_sim = max_min_rescale(y_em_sim, min_sim, max_sim)\n",
    "                    out_em_sim = max_min_rescale(out_em_sim, min_sim, max_sim)\n",
    "\n",
    "                    # generate semivariograms for labeled data                \n",
    "                    bins = [i / 10 for i in range(11)]\n",
    "                    thr = y_mask_pairs.shape[0] / len(bins) * 0.01\n",
    "\n",
    "                    dis_np = y_em_sim.detach().cpu().data.numpy() ** 0.5\n",
    "                    var_np = y_var.detach().cpu().data.numpy()                        \n",
    "                    popt = gen_semivariogram(dis_np, var_np, bins, thr)\n",
    "                    r, s, n = popt\n",
    "\n",
    "                    # generate semivariograms for unlabeled data\n",
    "    #                 dis_np1 = out_em_sim.detach().cpu().data.numpy() ** 0.5\n",
    "    #                 var_np1 = out_var.detach().cpu().data.numpy()\n",
    "    #                 popt1 = gen_semivariogram(dis_np1, var_np1, bins, thr)\n",
    "    #                 r1, s1, n1 = popt1    \n",
    "\n",
    "    #                 if 100 in batch_idx.detach().cpu().data.numpy() and epoch % 5 == 0:\n",
    "    #                     plt.figure(epoch, figsize=(10, 6))\n",
    "    #                     plt.scatter(dis_np, var_np, s=0.8, label='Label')\n",
    "    #                     plt.scatter(dis_np, gaussian(dis_np, *popt), s=0.8, label=f'Empirical Label : {r:.2f}, {s:.2f}, {n:.2f}')\n",
    "    # #                     plt.scatter(dis_np1, var_np1, s=0.8, label='Prediction')\n",
    "    #                     plt.scatter(dis_np1, gaussian(dis_np1, *popt1), s=0.8, label=f'Empirical Prediction : {r1:.2f}, {s1:.2f}, {n1:.2f}')\n",
    "    #                     plt.legend()\n",
    "    #                     plt.show()\n",
    "\n",
    "                    sub_loss = 0.\n",
    "                    valid_bins = [b for b in bins if b < r]                                      \n",
    "                    if r > 0 and s > 0 and n > 0 and n < s:\n",
    "                        for b in range(len(valid_bins) - 1):\n",
    "                            left, right = valid_bins[b], valid_bins[b + 1]\n",
    "                            mask1 = (y_em_sim >= left ** 2) & (y_em_sim < right ** 2)\n",
    "                            mask2 = (out_em_sim >= left ** 2) & (out_em_sim < right ** 2)\n",
    "                            if mask1.sum() > thr and mask2.sum() > thr:\n",
    "                                mu1, var1 = torch.mean(y_var[mask1]), torch.std(y_var[mask1])\n",
    "                                mu2, var2 = torch.mean(out_var[mask2]), torch.std(out_var[mask2])   \n",
    "                                if var1 > 0 and var2 > 0:\n",
    "                                    sub_loss += (torch.log(var2 ** 2 / var1 ** 2) - 1 + (var1 ** 2 + (mu1 - mu2) ** 2) / var2 ** 2) * 0.5   \n",
    "\n",
    "                    sub_loss = sub_loss / len(valid_bins) if len(valid_bins) > 0 else 0.\n",
    "                    total_loss += sub_loss * args.eta\n",
    "                        \n",
    "                    try:\n",
    "                        ac_losses.append(sub_loss.item())\n",
    "                    except:\n",
    "                        ac_losses.append(0.0)\n",
    "\n",
    "            total_losses.append(total_loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ######################\n",
    "            # validate the model #\n",
    "            ######################\n",
    "\n",
    "            val_loss = loss_func(batch_val_y[~torch.isnan(batch_val_y)], out[~torch.isnan(batch_val_y)])\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_total_loss = np.average(total_losses)\n",
    "        avg_train_loss = np.average(train_losses)\n",
    "        avg_val_loss = np.average(val_losses)\n",
    "        avg_l1_loss = np.average(l1_losses)\n",
    "        avg_ae_loss = np.average(ae_losses)\n",
    "        avg_ac_loss = np.average(ac_losses)\n",
    "        avg_tp_loss = np.average(tp_losses)\n",
    "        avg_sp_loss = np.average(sp_losses)\n",
    "        \n",
    "\n",
    "        # write for tensorboard visualization\n",
    "        writer.add_scalar('data/train_loss', avg_total_loss, epoch)\n",
    "        writer.add_scalar('data/val_loss', avg_val_loss, epoch)\n",
    "\n",
    "        logging.info(f'Epoch [{epoch}/{args.epochs}] total_loss = {avg_total_loss:.4f}, train_loss = {avg_train_loss:.4f}, valid_loss = {avg_val_loss:.4f}.')\n",
    "        logging.info(f'l1_loss = {avg_l1_loss:.4f}, ae_loss = {avg_ae_loss:.4f}, ac_loss = {avg_ac_loss:.4f}, tp_loss = {avg_tp_loss:.4f}, sp_loss = {avg_sp_loss:.4f}.')\n",
    "\n",
    "        ##################\n",
    "        # early_stopping #\n",
    "        ##################\n",
    "\n",
    "        early_stopping(avg_val_loss, dapm, kwargs['model_file'])\n",
    "\n",
    "        #########################\n",
    "        # evaluate testing data #\n",
    "        #########################\n",
    "        \n",
    "        if early_stopping.counter < 2 and epoch % 2 == 0:\n",
    "            \n",
    "            dapm.eval()\n",
    "            predictions = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(test_idx_data_loader):\n",
    "                    batch_idx = data[0]\n",
    "                    batch_x = construct_sequence_x(batch_idx, data_obj.dynamic_x, data_obj.static_x)  # x = (b, t, c, h, w)\n",
    "                    out, _, _, _, _ = dapm(batch_x)\n",
    "                    predictions.append(out.cpu().data.numpy())\n",
    "\n",
    "            prediction = np.concatenate(predictions)\n",
    "            rmse, mape, r2 = compute_error(data_obj.test_y[args.seq_len + 1:, ...], prediction)\n",
    "            writer.add_scalar('data/test_rmse', rmse, epoch)\n",
    "            logging.info(f'Testing: RMSE = {rmse:.4f}, MAPE = {mape:.4f}, R2 = {r2:.4f}.')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            logging.info(kwargs['model_name'] + f' val_loss = {early_stopping.val_loss_min:.4f}.')\n",
    "            logging.info('Early stopping')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    define directory\n",
    "\"\"\"\n",
    "\n",
    "base_dir = f'data/los_angeles_500m_acc_1234_tp1_2/'\n",
    "train_val_test_file = f'/home/yijun/notebooks/training_data/train_val_test_los_angeles_500m_fine_tune_1234.json'\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else 'cpu')  # the gpu device\n",
    "\n",
    "\"\"\" load train, val, test locations \"\"\"\n",
    "f = open(train_val_test_file, 'r')\n",
    "train_val_test = json.loads(f.read())\n",
    "\n",
    "kwargs = {\n",
    "    'model_dir': os.path.join(base_dir, 'models1/'),\n",
    "    'log_dir': os.path.join(base_dir, 'logs1/'),\n",
    "    'run_dir': os.path.join(base_dir, 'runs/'),\n",
    "    'train_val_test': train_val_test,\n",
    "    'device': device\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#02#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#01#___6_00001_1___1_20_5_01___16_13.pkl\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#03#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#02#___6_00001_1___1_10_5_01___16_13.pkl\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#04#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#03#___6_00001_1___1_10_5_01___16_13.pkl\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#05#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#04#___6_00001_1___1_10_5_01___16_13.pkl\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#06#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#05#___6_00001_1___1_10_5_01___16_13.pkl\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#07#___6_00001_1___1_10_5_01___16_13\n",
      "dapm___sp_ae_esc_acc___los_angeles_500m_2018___#06#___6_00001_1___1_10_5_01___16_13.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0bc0f39f6f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp_neighbor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'esc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdapm_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9b7f59e52347>\u001b[0m in \u001b[0;36mdapm_main\u001b[0;34m(param, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#         p.requires_grad = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;34m\"\"\" logging ends \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-770b24a1b739>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dapm, data_obj, args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(2, 13):\n",
    "\n",
    "    param = Param([i], 2018, alpha=1, beta=10, gamma=5, eta=0.1, sp_neighbor=1, lr=0.0002, batch_size=8, model_type=['sp', 'ae', 'esc', 'acc'])\n",
    "    dapm_main(param, **kwargs)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
